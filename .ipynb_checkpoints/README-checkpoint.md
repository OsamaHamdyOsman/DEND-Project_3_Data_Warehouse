# Project: Data Warehouse
## Overview:
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The main goal of this project is to create an ETL pipline to move data to the cloud on a datawarehous on Amazon Redshift.

## Datasets
Sparkify provided two datasets that reside in S3. Here are the S3 links for each:

* `Song data: s3://udacity-dend/song_data`
* `Log data: s3://udacity-dend/log_data`

### song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

`song_data/A/B/C/TRABCEI128F424C983.json`
`song_data/A/A/B/TRAABJL12903CDCF1A.json`
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

### Log Dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset are partitioned by year and month.

* ***Sample Data:***

`log_data/2018/11/2018-11-12-events.json`

`log_data/2018/11/2018-11-13-events.json`

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

<img src='log-data.png' width='700'>

## Star Schema for Song Play

<img src='Erd.png' width='700'>

### Fact Table
1. ***songplays*** - records in event data associated with song plays i.e. records with page `NextSong`

* Fields
`songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent`

### Dimension Tables
2. ***users*** - users in the app

* Fields
`user_id, first_name, last_name, gender, level`

3. ***songs*** - songs in music database

* Fields
`song_id, title, artist_id, year, duration`

4. ***artists*** - artists in music database

* Fields
`artist_id, name, location, lattitude, longitude`

5. ***time*** - timestamps of records in ***songplays*** broken down into specific units

* Fields
`start_time, hour, day, week, month, year, weekday`

## Way to Go:

### Redshift Cluster Launch

In the **`AWS_cluster_creation.ipynb`** using the same process illustated in the course material ***L3 Exercise 2 - IaC***. Infrastructure as code to launch the cluster. After creating the cluster, some queries were utilized to test the database.

### Configuration

The ***`dwh.cfg`*** is the configuration file that is required to get the cluster launched and the data loaded. You can use your own parameters, keys and authentication to get the file ready for work.

### Code for Data Loading and Tables creation:

**`sql_queries.py`** is where you'll define you SQL statements, which will be imported into the two following files.

### Data Loading:

The **`etl.py`** is where you'll load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift.

### Creating the Star Schema:

**`create_table.py`** is where you'll create your fact and dimension tables for the star schema in Redshift.

### The Actions to be taked

After writing the sql queries to load the data and create the tables in the `sql_queries.py`, run the following commands on the terminal:

`python create_tables.py` to create the tables

`python etl.py` to load the data into the created tables.

### Final Word on Distribution Styles:
After creating the tables, i noticed the size of each table and adjusted the Distribution style in accordance. 

1. The `songplays` table:
Despite being the fact table yet it is comprised of 333 rows only. So, i opted for the DISTSTYLE ALL for this small table, to be modified in the future when data accumilates in it.

2. The `users` table:
A 104 rows table also was set to DISTSTYLE ALL

3. The `songs` table:
The largest in the star schema of 14896 rows, was set to DISTSTYLE KEY using the `artist_id` as DISTKEY and SORTKEY.

4. The `artists` table: 
The second largest table with 10025 rows, was set to DISTSTYLE KEY using the `artist_id` as DISTKEY and SORTKEY. In such way if any query involved the `songs`  and the `artists` tables, it's supposed to be efficient without shuffling. 

5. The `time` table:
Also was set to DISTSTYLE as it contains some 8023 rows. using the `start_time` as a SORTKEY as it's usually the case the sort by timestamps.

